
<!doctype html>
<!-- START OF _layouts/default.html -->
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta content="IE=edge,chrome=1" http-equiv="X-UA-Compatible" >
		<meta content="width=device-width,initial-scale=1" name="viewport">
		<meta content="Andrew Hao's thoughts on software engineering, team leadership & product design." name="description">
		<meta content="The Sweet Spot" name="author">

		<title>Tensorflow for Tears: Part 2 &mdash; The Sweet Spot</title>

		<!-- Styles -->
		<link href="/stylesheets/main.css" rel="stylesheet">

		<!-- Google webfonts -->
    <link href="https://fonts.googleapis.com/css?family=Cousine|Suez+One|Yellowtail|Space+Mono|Alike+Angular" rel="stylesheet">

		<!-- jQuery and plugins -->
		<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.2/jquery.min.js"></script>
		<script src="/js/jquery.zclip.min.js"></script>

		<!-- Syntax highlighter -->
		<link href="/stylesheets/prettify-hemisu.css" type="text/css" rel="stylesheet" />
    <script type="text/javascript" src="/js/prettify.js"></script>

    <link href="" rel="alternate" title="The Sweet Spot" type="application/atom+xml">
    


  </head>

	<body onload="prettyPrint()">
		<div class="wrap">

			<header>
        <a href="/"><div class="title">The Sweet Spot</div></a>
				<div class="subtitle">On software, engineering leadership, and anything shiny.</div>

				<div class="navi">
					<ul>
						<li><a href="/">Articles</a></li>
						<li><a href="/talks">Talks</a></li>
						<li><a href="/notes">Conference Notes</a></li>
						<li><a href="https://www.github.com/andrewhao">Github</a></li>
						<li><a href="https://twitter.com/andrewhao">Twitter</a></li>
					</ul>
				</div> <!-- // .navi -->
			</header>

				<!-- START OF _layouts/post.html -->

<!-- START OF _includes/article.html -->
<article>
	<header>
		<h1><a href="/2018/08/27/tensorflow-for-tears-part-2/">Tensorflow for Tears: Part 2</a></h1>
		<time>27 August 2018</time>
	</header>
		<div class="content">
			<p>The <a href="/2018/03/21/tensorflow-for-tears-part-1/">last time we met</a>, we had a lively discussion about the ins and outs and joys and terrors of parenting. I talked about how I started building a Raspberry Pi project with a USB mic and wrote a simple parsing script that measured the mean amplitude of recordings of the current state of the nursery. And when that guy wailed, he really WAILED.</p>

<p>Well that naive approach got us so far, but the system would still trip up due to random loud noises in the house. Music, or doors closing or opening, or loud conversation would all cause the system to think the kid was crying but no - it was just ambient noise.</p>

<h3 id="getting-started-with-tensorflow">Getting started with TensorFlow</h3>

<p>Let’s start our dive into machine learning!</p>

<p>I had already gotten pretty far with <a href="https://www.udacity.com/course/intro-to-machine-learning--ud120">Udacity’s Machine Learning</a> course and had vague recollections of AI theory floating around from the cobwebs of undergrad CS courses past. So I had <em>some</em> background in AI and machine learning. But training neural networks was a completely new thing to me.</p>

<p>Luckily, I came across the <a href="https://www.tensorflow.org/tutorials/sequences/audio_recognition">Simple Audio Recognition Tutorial</a> example right there on the TF homepage. This was exactly what I was looking for. The objective was: given a training set of audio clips of crying babies and “empty rooms”, classify an audio clip as one or the other.</p>

<h3 id="first-getting-the-dataset">First: getting the dataset</h3>

<p>I had oversimplified in my mind what made a great training data set. After all, I figured that I just had to record my kid crying a bunch, then get a few minutes of quiet room sounds, and then we were good, right?</p>

<p>Wrong.</p>

<p>Training data must be exactly matched. Sample rates must be consistent, and audio samples must either be highly randomized or at least normalized to the same rates. To wit, here’s how I gathered my sample data:</p>

<ol>
  <li>I set up the Raspberry Pi to continuously record 30-second samples at 22.05kHz through the onboard microphone.</li>
  <li>When my son started a crying episode, I’d make note of the start time and end times, and then go back and gather those clips of the cries and dump them in the <code class="highlighter-rouge">crying</code> folder.</li>
  <li>Once I had collected enough of those (maybe 30 minutes total), I then gathered a bunch of the “other” clips of quiet sleep in his nursery and threw these in the <code class="highlighter-rouge">silence</code> folder.</li>
  <li>Once I had moved these samples off the Raspberry Pi and onto my laptop, I then sliced these recordings into 5-second clips using <code class="highlighter-rouge">sox</code>. I also applied a few amplification filters to overcome the weak pickup on the mic.</li>
</ol>

<p><code class="highlighter-rouge">$ sox FILENAME FILENAME_OUTPUT trim 0 5 vol 45 dB rate 22050</code></p>

<p>I then placed each of these trained samples in the folders corresponding to their label: <code class="highlighter-rouge">crying</code> and <code class="highlighter-rouge">silence</code>.</p>

<h3 id="second-training-the-network">Second: training the network</h3>

<p>I modified the <code class="highlighter-rouge">train.py</code> script nearly verbatim from TF docs. We’ll dissect it here, beginning with the command to begin training:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="nv">$ </span>python app/train.py <span class="nt">--data_url</span><span class="o">=</span> <span class="nt">--data_dir</span><span class="o">=</span>./data <span class="nt">--wanted_words</span><span class="o">=</span>silence,crying <span class="nt">--sample_rate</span><span class="o">=</span>22050 <span class="nt">--clip_duration_ms</span><span class="o">=</span>5000 <span class="nt">--how_many_training_steps</span><span class="o">=</span>1000,200 <span class="nt">--learning_rate</span><span class="o">=</span>0.001,0.0001 <span class="nt">--train_dir</span><span class="o">=</span>./training
</code></pre></div></div>

<ul>
  <li><code class="highlighter-rouge">--wanted_words=silence,crying</code>: This specifies which labels should be considered for training purposes.</li>
  <li><code class="highlighter-rouge">--sample_rate</code>: The sample rate of the audio files provided</li>
  <li><code class="highlighter-rouge">--clip_duration_ms</code>: Duration of each training clip in milliseconds</li>
  <li><code class="highlighter-rouge">--how_many_training_steps</code>: This is a comma-separated list of n numbers that specify the number of steps per phase.</li>
  <li><code class="highlighter-rouge">--learning_rate</code>: This is the rate at which the system can adjust its current learnings to match its new inputs. A higher learning rate means the faster the system can change to learn new inputs. A lower number ensures the stability of a system’s learning. We specify a higher training rate for the first phase and lower the training rate in the latter phase as our precision increases.</li>
</ul>

<p>Let’s run the script!</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>python app/train.py <span class="nt">--data_url</span><span class="o">=</span> <span class="nt">--data_dir</span><span class="o">=</span>./data <span class="nt">--wanted_words</span><span class="o">=</span>silence,crying <span class="nt">--sample_rate</span><span class="o">=</span>22050 <span class="nt">--clip_duration_ms</span><span class="o">=</span>5000 <span class="nt">--how_many_training_steps</span><span class="o">=</span>1000,200 <span class="nt">--train_dir</span><span class="o">=</span>./training
2018-08-27 22:15:33.195894: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Tensor<span class="o">(</span><span class="s2">"Placeholder:0"</span>, <span class="nv">shape</span><span class="o">=()</span>, <span class="nv">dtype</span><span class="o">=</span>string<span class="o">)</span>
INFO:tensorflow:Training from step: 1
INFO:tensorflow:Step <span class="c">#1: rate 0.001000, accuracy 26.0%, cross entropy 2.674081</span>
INFO:tensorflow:Step <span class="c">#2: rate 0.001000, accuracy 23.0%, cross entropy 1.593786</span>
INFO:tensorflow:Step <span class="c">#3: rate 0.001000, accuracy 64.0%, cross entropy 1.067298</span>
INFO:tensorflow:Step <span class="c">#4: rate 0.001000, accuracy 73.0%, cross entropy 0.843605</span>
...
</code></pre></div></div>

<h3 id="third-parsing-the-results">Third: parsing the results</h3>

<p>The output in each step reveals the current state of the neural network as trained. <em>Accuracy</em> reflects the correctness of the model as the validation set is tested against the network (during each step of training, samples in the validation set are run against the model and checked if they line up with their specified labels).</p>

<p><a href="https://www.tensorflow.org/api_docs/python/tf/losses/softmax_cross_entropy"><em>Cross entropy</em></a> is, as I understand it, the <a href="https://deepnotes.io/softmax-crossentropy">squared error factor</a> in the result network from the actual results (lower is better).</p>

<p>This would proceed for several hours for 1200 total steps. On a 2013-era Macbook Pro, this took approximately 6 hours. I had 350 clips of crying and 500 clips of silence. (Too much or not enough? This tired parent says “too much”.)</p>

<p>One more thing - every few hundred steps during training, we would get this sort of output:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>INFO:tensorflow:Confusion Matrix:
 [[ 9  0  0  0]
 [ 0  0  0  0]
 [ 0  0 55  0]
 [ 0  0  1 30]]
</code></pre></div></div>

<p>What’s a <a href="https://www.tensorflow.org/api_docs/python/tf/confusion_matrix">confusion matrix</a>? It’s, according to <a href="https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/">this helpful article</a>, another way to visualize the accuracy of a machine learning model.</p>

<p>Here’s how to read this confusion matrix. Given the following labels:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># conv_labels.txt
_silence_
_unknown_
silence
crying
</code></pre></div></div>

<p><em>(Where did these come from? More on that later…)</em></p>

<p>Imagine these labels go left-to-right, and top-to-bottom. The x-axis represents the labels that have been predicted (i.e. that have been verified) to be a certain label. So the first column represents the percentages of samples that have been predicted to be <code class="highlighter-rouge">_silence_</code>.</p>

<p>The labels that go top-to-bottom are the actual results from the trained model. So in this case, if we take the top-left number <code class="highlighter-rouge">9</code>, that means that in 9 runs of the model where the prediction was <code class="highlighter-rouge">_silence_</code>, the actual result was <code class="highlighter-rouge">_silence_</code>. Moving one cell down, that represents the # of samples where the predicted result was <code class="highlighter-rouge">_silence_</code>, but the actual result was <code class="highlighter-rouge">_unknown_</code>. Fortunately for our model, there are <code class="highlighter-rouge">0</code> results in this cell. So on and so forth. So the ideal confusion matrix is a matrix that has a “diagonal line” running from top-left to bottom-right, and <code class="highlighter-rouge">0</code>s everywhere else, because all predictions would equal actuals.</p>

<p>tl;dr: Confusion matrices are a way to visualize and report the accuracy of a machine learning model. You want a clear and convincing diagonal line in the matrix.</p>

<p>Oh, and here are the results from the training data. I’m using <code class="highlighter-rouge">tensorboard</code> to visualize the training steps:</p>

<p><img src="/images/tensorflow-for-tears/tensorboard-accuracy.png" alt="Accuracy data" />
<em>Accuracy modeling. Note how quickly the model jumps to be fairly accurate.</em></p>

<p><img src="/images/tensorflow-for-tears/tensorboard-cross-entropy.png" alt="Cross entropy data" />
<em>Note how quickly cross entropy dives.</em></p>

<p>We can use these graphs to tune our models if we really cared. In this case, I say it’s good enough (accuracy is up to 99% by the end).</p>

<h3 id="fourth-saving-and-exporting-the-model">Fourth: Saving and exporting the model</h3>

<p>OK, but enough already. We have a trained model and, like <a href="https://en.wikipedia.org/wiki/Chekhov%27s_gun">Chekhov’s Gun</a>, that means we’ve gotta use it!</p>

<p>Where’s that model? Oh, it needs a few more steps before it can emerge. At this point, TensorFlow has developed a neural network, but the neuron graph (is that the right term?) is not yet in a usable state to be used by applications. To that point, we need to dump the model into a binary format that can be used by TensorFlow applications in the future.</p>

<p>Once again, I claim no smarts in all this, but instead point to the TensorFlow script to do this in <code class="highlighter-rouge">app/freeze.py</code>:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ python app/freeze.py --start_checkpoint=./training/conv.ckpt-1200 --output_file=./graph.pb --clip_duration_ms=5000 --sample_rate=22050 --wanted_words=silence,crying --data_dir=./data
</code></pre></div></div>

<p>What did we specify here?</p>

<p>We said we wanted the model at <code class="highlighter-rouge">--start_checkpoint</code> of 1200, saving the <code class="highlighter-rouge">--output_file</code> to <code class="highlighter-rouge">graph.pb</code>, and mentioning that the sample rate of each audio sample should be <code class="highlighter-rouge">22050 hz</code> and <code class="highlighter-rouge">5</code> seconds long. We then specified that the labels we wanted to classify are <code class="highlighter-rouge">silence</code> and <code class="highlighter-rouge">crying</code>. Finally, the data set from the prior run can be found in <code class="highlighter-rouge">./data</code> dir.</p>

<p>When we run this script, we get a <code class="highlighter-rouge">graph.pb</code> protobuf binary file that we can then ship to various TensorFlow programs.</p>

<h3 id="fifth-using-the-model">Fifth: Using the model</h3>

<p>Now here’s the fun part!</p>

<p>Onboard a Raspberry Pi, we are now going to play back current samples in the nursery:</p>

<p>Every 1 minute (with cron), we record audio samples from the system mic in the baby’s nursery. We then massage, crop and downsample it into a WAV file. We then point a script at this WAV file and run the TF graph on it. Running the graph will return a list of labels and their probabilities. We choose the first label with the highest probability, and ship it off to a timeseries API, in this case powered by Keen.io.</p>

<p>Voila:</p>

<p><img src="/images/tensorflow-for-tears/miserymeter.png" alt="I present: the Misery Meter" />
<em>A fun graph displaying the timeseries data for this little dude’s episodes. On the top was the original RMS volume graph, and the bottom is the result of the trained TF model. Note how much easier to read and understand the latter graph is.</em></p>

<h4 id="show-me-the-code">Show me the code!</h4>

<p>Much of this code has been adapted from <a href="https://www.tensorflow.org/tutorials/sequences/audio_recognition">Google’s TensorFlow Audio Recognition tutorial</a>.</p>

<p>My scripts have been collected on this GitHub repository: <a href="https://github.com/andrewhao/babblefish">https://github.com/andrewhao/babblefish</a></p>

<p>And my repository with audio sampling and archiving: <a href="https://github.com/andrewhao/miserymeter">https://github.com/andrewhao/miserymeter</a></p>

<h2 id="conclusion">Conclusion</h2>

<p>Wow, that was a quick dive through TensorFlow. Note that I didn’t get too deep into the theory of convolutional neural networks, which may be a topic of discussion for another time. Instead, we talked a little bit about the mechanics of building and training a TF model with audio data and a finite set of classes. It was fairly straightforward to get this script then loaded up on a Raspberry Pi and have a dashboard that could finally quantify the pain and suffering of baby… and parent.</p>

<h3 id="epilogue">Epilogue</h3>

<p>The months that went into developing this app were fully and knowingly an escape from the very real stressors of parent life. I want to acknowledge the love, the grit and the patience of my wife in this very trying time. There is so much more to say about babies other than their crying and fussing - life these days is filled with laughter and giggles and joy, too - but these are words that I’ll save for another blog entry on a different blog for another time.</p>

			
			
		</div>
	<footer>
		<div class="article__categories">
  Tagged under
  
  
  <a class="article__category" href="/category/tensorflow">TensorFlow</a>
  , 
  
  <a class="article__category" href="/category/machine-learning">Machine Learning</a>
  , 
  
  <a class="article__category" href="/category/parenthood">Parenthood</a>
  
  
</div>

	
	
	</footer>
	
</article>
<!-- END OF _includes/article.html -->


  <section class="comments">
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
  </section>


<!-- END OF _/layouts/post.html -->


			<footer>
				Copyright &copy; 2018

	The Sweet Spot


				

<script type="text/javascript">
      var disqus_shortname = 'g9labs';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://localhost:4000/2018/08/27/tensorflow-for-tears-part-2/';
        var disqus_url = 'http://localhost:4000/2018/08/27/tensorflow-for-tears-part-2/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>











			</footer>

		</div> <!-- // .wrap -->
  </body>

	<script>
		$(document).ready(function() {
			// Make images center
			$('p:has(img)').css('text-align', 'center');

			// Add the image's title attribute as a caption
			$('p:has(img)').append(function () {
				return '<div class="caption">' + ($('img', this).attr('title') || "") + '</div>';
			});

			// Prettify code
			$('code').addClass('prettyprint');
			$('pre code').addClass('linenums');

			// Copy to clipboard with button
			$('pre:has(code)').prepend(function(){
				return '<div class="clip-btn">copy to clipboard</div>';
      });

			$('.clip-btn').zclip({
				path:'/js/ZeroClipboard.swf',
				copy: $(this).next('code').text(),
				afterCopy: function(){
					$(this).replaceWith('<div class="clip-btn">copied!');
					}
			});
		});
	</script>
</html>
<!-- END OF _layouts/default.html -->
