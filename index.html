
<!doctype html>
<!-- START OF _layouts/default.html -->
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta content="IE=edge,chrome=1" http-equiv="X-UA-Compatible" >
		<meta content="width=device-width,initial-scale=1" name="viewport">
		<meta content="Andrew Hao's thoughts on software engineering, team leadership & product design." name="description">
		<meta content="The Sweet Spot" name="author">

		<title>The Sweet Spot</title>

		<!-- Styles -->
		<link href="/stylesheets/main.css" rel="stylesheet">

		<!-- Google webfonts -->
    <link href="https://fonts.googleapis.com/css?family=Cousine|Suez+One|Yellowtail|Space+Mono|Alike+Angular" rel="stylesheet">

		<!-- jQuery and plugins -->
		<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.2/jquery.min.js"></script>
		<script src="/js/jquery.zclip.min.js"></script>

		<!-- Syntax highlighter -->
		<link href="/stylesheets/prettify-hemisu.css" type="text/css" rel="stylesheet" />
    <script type="text/javascript" src="/js/prettify.js"></script>

    <link href="/atom.xml" rel="alternate" title="The Sweet Spot" type="application/atom+xml">
    
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-2330913-9']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>



  </head>

	<body onload="prettyPrint()">
		<div class="wrap">

			<header>
        <a href="/"><div class="title">The Sweet Spot</div></a>
				<div class="subtitle">On software, engineering leadership, and anything shiny.</div>

				<div class="navi">
					<ul>
						<li><a href="/blog/archives">Articles</a></li>
						<li><a href="/talks">Talks</a></li>
						<li><a href="/notes">Conference Notes</a></li>
						<li><a href="https://www.github.com/andrewhao">Github</a></li>
						<li><a href="https://twitter.com/andrewhao">Twitter</a></li>
					</ul>
				</div> <!-- // .navi -->
			</header>

				<!-- START OF index.html -->

	
		
		<!-- START OF _includes/article.html -->
<article>
	<header>
		<h1><a href="/2018/08/27/tensorflow-for-tears-part-2/">Tensorflow for Tears: Part 2</a></h1>
		<time>27 August 2018</time>
	</header>
		<div class="content">
			<p>The <a href="/2018/03/21/tensorflow-for-tears-part-1/">last time we met</a>, we had a lively discussion about the ins and outs and joys and terrors of parenting. I talked about how I started building a Raspberry Pi project with a USB mic and wrote a simple parsing script that measured the mean amplitude of recordings of the current state of the nursery. And when that guy wailed, he really WAILED.</p>

<p>Well that naive approach got us so far, but the system would still trip up due to random loud noises in the house. Music, or doors closing or opening, or loud conversation would all cause the system to think the kid was crying but no - it was just ambient noise.</p>

<h3 id="getting-started-with-tensorflow">Getting started with TensorFlow</h3>

<p>Let’s start our dive into machine learning!</p>

<p>I had already gotten pretty far with <a href="https://www.udacity.com/course/intro-to-machine-learning--ud120">Udacity’s Machine Learning</a> course and had vague recollections of AI theory floating around from the cobwebs of undergrad CS courses past. So I had <em>some</em> background in AI and machine learning. But training neural networks was a completely new thing to me.</p>

<p>Luckily, I came across the <a href="https://www.tensorflow.org/tutorials/sequences/audio_recognition">Simple Audio Recognition Tutorial</a> example right there on the TF homepage. This was exactly what I was looking for. The objective was: given a training set of audio clips of crying babies and “empty rooms”, classify an audio clip as one or the other.</p>

<h3 id="first-getting-the-dataset">First: getting the dataset</h3>

<p>I had oversimplified in my mind what made a great training data set. After all, I figured that I just had to record my kid crying a bunch, then get a few minutes of quiet room sounds, and then we were good, right?</p>

<p>Wrong.</p>

<p>Training data must be exactly matched. Sample rates must be consistent, and audio samples must either be highly randomized or at least normalized to the same rates. To wit, here’s how I gathered my sample data:</p>

<ol>
  <li>I set up the Raspberry Pi to continuously record 30-second samples at 22.05kHz through the onboard microphone.</li>
  <li>When my son started a crying episode, I’d make note of the start time and end times, and then go back and gather those clips of the cries and dump them in the <code>crying</code> folder.</li>
  <li>Once I had collected enough of those (maybe 30 minutes total), I then gathered a bunch of the “other” clips of quiet sleep in his nursery and threw these in the <code>silence</code> folder.</li>
  <li>Once I had moved these samples off the Raspberry Pi and onto my laptop, I then sliced these recordings into 5-second clips using <code>sox</code>. I also applied a few amplification filters to overcome the weak pickup on the mic.</li>
</ol>

<p><code>$ sox FILENAME FILENAME_OUTPUT trim 0 5 vol 45 dB rate 22050</code></p>

<p>I then placed each of these trained samples in the folders corresponding to their label: <code>crying</code> and <code>silence</code>.</p>

<h3 id="second-training-the-network">Second: training the network</h3>

<p>I modified the <code>train.py</code> script nearly verbatim from TF docs. We’ll dissect it here, beginning with the command to begin training:</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line"><span class="nv">$ </span>python app/train.py --data_url<span class="o">=</span> --data_dir<span class="o">=</span>./data --wanted_words<span class="o">=</span>silence,crying --sample_rate<span class="o">=</span><span class="m">22050</span> --clip_duration_ms<span class="o">=</span><span class="m">5000</span> --how_many_training_steps<span class="o">=</span>1000,200 --learning_rate<span class="o">=</span>0.001,0.0001 --train_dir<span class="o">=</span>./training
</span></code></pre></td></tr></table></div></figure>

<ul>
  <li><code>--wanted_words=silence,crying</code>: This specifies which labels should be considered for training purposes.</li>
  <li><code>--sample_rate</code>: The sample rate of the audio files provided</li>
  <li><code>--clip_duration_ms</code>: Duration of each training clip in milliseconds</li>
  <li><code>--how_many_training_steps</code>: This is a comma-separated list of n numbers that specify the number of steps per phase.</li>
  <li><code>--learning_rate</code>: This is the rate at which the system can adjust its current learnings to match its new inputs. A higher learning rate means the faster the system can change to learn new inputs. A lower number ensures the stability of a system’s learning. We specify a higher training rate for the first phase and lower the training rate in the latter phase as our precision increases.</li>
</ul>

<p>Let’s run the script!</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line"><span class="nv">$ </span>python app/train.py --data_url<span class="o">=</span> --data_dir<span class="o">=</span>./data --wanted_words<span class="o">=</span>silence,crying --sample_rate<span class="o">=</span><span class="m">22050</span> --clip_duration_ms<span class="o">=</span><span class="m">5000</span> --how_many_training_steps<span class="o">=</span>1000,200 --train_dir<span class="o">=</span>./training
</span><span class="line">2018-08-27 22:15:33.195894: I tensorflow/core/platform/cpu_feature_guard.cc:140<span class="o">]</span> Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
</span><span class="line">Tensor<span class="o">(</span><span class="s2">&quot;Placeholder:0&quot;</span>, <span class="nv">shape</span><span class="o">=()</span>, <span class="nv">dtype</span><span class="o">=</span>string<span class="o">)</span>
</span><span class="line">INFO:tensorflow:Training from step: 1
</span><span class="line">INFO:tensorflow:Step <span class="c">#1: rate 0.001000, accuracy 26.0%, cross entropy 2.674081</span>
</span><span class="line">INFO:tensorflow:Step <span class="c">#2: rate 0.001000, accuracy 23.0%, cross entropy 1.593786</span>
</span><span class="line">INFO:tensorflow:Step <span class="c">#3: rate 0.001000, accuracy 64.0%, cross entropy 1.067298</span>
</span><span class="line">INFO:tensorflow:Step <span class="c">#4: rate 0.001000, accuracy 73.0%, cross entropy 0.843605</span>
</span><span class="line">...
</span></code></pre></td></tr></table></div></figure>

<h3 id="third-parsing-the-results">Third: parsing the results</h3>

<p>The output in each step reveals the current state of the neural network as trained. <em>Accuracy</em> reflects the correctness of the model as the validation set is tested against the network (during each step of training, samples in the validation set are run against the model and checked if they line up with their specified labels).</p>

<p><a href="https://www.tensorflow.org/api_docs/python/tf/losses/softmax_cross_entropy"><em>Cross entropy</em></a> is, as I understand it, the <a href="https://deepnotes.io/softmax-crossentropy">squared error factor</a> in the result network from the actual results (lower is better).</p>

<p>This would proceed for several hours for 1200 total steps. On a 2013-era Macbook Pro, this took approximately 6 hours. I had 350 clips of crying and 500 clips of silence. (Too much or not enough? This tired parent says “too much”.)</p>

<p>One more thing - every few hundred steps during training, we would get this sort of output:</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">INFO:tensorflow:Confusion Matrix:
</span><span class="line"> <span class="o">[[</span> <span class="m">9</span>  <span class="m">0</span>  <span class="m">0</span>  0<span class="o">]</span>
</span><span class="line"> <span class="o">[</span> <span class="m">0</span>  <span class="m">0</span>  <span class="m">0</span>  0<span class="o">]</span>
</span><span class="line"> <span class="o">[</span> <span class="m">0</span>  <span class="m">0</span> <span class="m">55</span>  0<span class="o">]</span>
</span><span class="line"> <span class="o">[</span> <span class="m">0</span>  <span class="m">0</span>  <span class="m">1</span> 30<span class="o">]]</span>
</span></code></pre></td></tr></table></div></figure>

<p>What’s a <a href="https://www.tensorflow.org/api_docs/python/tf/confusion_matrix">confusion matrix</a>? It’s, according to <a href="https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/">this helpful article</a>, another way to visualize the accuracy of a machine learning model.</p>

<p>Here’s how to read this confusion matrix. Given the following labels:</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line"><span class="c"># conv_labels.txt</span>
</span><span class="line">_silence_
</span><span class="line">_unknown_
</span><span class="line">silence
</span><span class="line">crying
</span></code></pre></td></tr></table></div></figure>

<p><em>(Where did these come from? More on that later…)</em></p>

<p>Imagine these labels go left-to-right, and top-to-bottom. The x-axis represents the labels that have been predicted (i.e. that have been verified) to be a certain label. So the first column represents the percentages of samples that have been predicted to be <code>_silence_</code>.</p>

<p>The labels that go top-to-bottom are the actual results from the trained model. So in this case, if we take the top-left number <code>9</code>, that means that in 9 runs of the model where the prediction was <code>_silence_</code>, the actual result was <code>_silence_</code>. Moving one cell down, that represents the # of samples where the predicted result was <code>_silence_</code>, but the actual result was <code>_unknown_</code>. Fortunately for our model, there are <code>0</code> results in this cell. So on and so forth. So the ideal confusion matrix is a matrix that has a “diagonal line” running from top-left to bottom-right, and <code>0</code>s everywhere else, because all predictions would equal actuals.</p>

<p>tl;dr: Confusion matrices are a way to visualize and report the accuracy of a machine learning model. You want a clear and convincing diagonal line in the matrix.</p>

<p>Oh, and here are the results from the training data. I’m using <code>tensorboard</code> to visualize the training steps:</p>

<p><img src="/images/tensorflow-for-tears/tensorboard-accuracy.png" alt="Accuracy data" /><br />
<em>Accuracy modeling. Note how quickly the model jumps to be fairly accurate.</em></p>

<p><img src="/images/tensorflow-for-tears/tensorboard-cross-entropy.png" alt="Cross entropy data" /><br />
<em>Note how quickly cross entropy dives.</em></p>

<p>We can use these graphs to tune our models if we really cared. In this case, I say it’s good enough (accuracy is up to 99% by the end).</p>

<h3 id="fourth-saving-and-exporting-the-model">Fourth: Saving and exporting the model</h3>

<p>OK, but enough already. We have a trained model and, like <a href="https://en.wikipedia.org/wiki/Chekhov%27s_gun">Chekhov’s Gun</a>, that means we’ve gotta use it!</p>

<p>Where’s that model? Oh, it needs a few more steps before it can emerge. At this point, TensorFlow has developed a neural network, but the neuron graph (is that the right term?) is not yet in a usable state to be used by applications. To that point, we need to dump the model into a binary format that can be used by TensorFlow applications in the future.</p>

<p>Once again, I claim no smarts in all this, but instead point to the TensorFlow script to do this in <code>app/freeze.py</code>:</p>

<pre><code>$ python app/freeze.py --start_checkpoint=./training/conv.ckpt-1200 --output_file=./graph.pb --clip_duration_ms=5000 --sample_rate=22050 --wanted_words=silence,crying --data_dir=./data
</code></pre>

<p>What did we specify here?</p>

<p>We said we wanted the model at <code>--start_checkpoint</code> of 1200, saving the <code>--output_file</code> to <code>graph.pb</code>, and mentioning that the sample rate of each audio sample should be <code>22050 hz</code> and <code>5</code> seconds long. We then specified that the labels we wanted to classify are <code>silence</code> and <code>crying</code>. Finally, the data set from the prior run can be found in <code>./data</code> dir.</p>

<p>When we run this script, we get a <code>graph.pb</code> protobuf binary file that we can then ship to various TensorFlow programs.</p>

<h3 id="fifth-using-the-model">Fifth: Using the model</h3>

<p>Now here’s the fun part!</p>

<p>Onboard a Raspberry Pi, we are now going to play back current samples in the nursery:</p>

<p>Every 1 minute (with cron), we record audio samples from the system mic in the baby’s nursery. We then massage, crop and downsample it into a WAV file. We then point a script at this WAV file and run the TF graph on it. Running the graph will return a list of labels and their probabilities. We choose the first label with the highest probability, and ship it off to a timeseries API, in this case powered by Keen.io.</p>

<p>Voila:</p>

<p><img src="/images/tensorflow-for-tears/miserymeter.png" alt="I present: the Misery Meter" /><br />
<em>A fun graph displaying the timeseries data for this little dude’s episodes. On the top was the original RMS volume graph, and the bottom is the result of the trained TF model. Note how much easier to read and understand the latter graph is.</em></p>

<h4 id="show-me-the-code">Show me the code!</h4>

<p>Much of this code has been adapted from <a href="https://www.tensorflow.org/tutorials/sequences/audio_recognition">Google’s TensorFlow Audio Recognition tutorial</a>.</p>

<p>My scripts have been collected on this GitHub repository: https://github.com/andrewhao/babblefish</p>

<p>And my repository with audio sampling and archiving: https://github.com/andrewhao/miserymeter</p>

<h2 id="conclusion">Conclusion</h2>

<p>Wow, that was a quick dive through TensorFlow. Note that I didn’t get too deep into the theory of convolutional neural networks, which may be a topic of discussion for another time. Instead, we talked a little bit about the mechanics of building and training a TF model with audio data and a finite set of classes. It was fairly straightforward to get this script then loaded up on a Raspberry Pi and have a dashboard that could finally quantify the pain and suffering of baby… and parent.</p>

<h3 id="epilogue">Epilogue</h3>

<p>The months that went into developing this app were fully and knowingly an escape from the very real stressors of parent life. I want to acknowledge the love, the grit and the patience of my wife in this very trying time. There is so much more to say about babies other than their crying and fussing - life these days is filled with laughter and giggles and joy, too - but these are words that I’ll save for another blog entry on a different blog for another time.</p>

			
			
		</div>
	<footer>
		


		



  
		
			Tagged under
		
    <a class='category' href='/category/machine-learning/'>machine learning</a>, <a class='category' href='/category/parenthood/'>parenthood</a>, <a class='category' href='/category/tensorflow/'>tensorflow</a>
	


	
	
	</footer>
	
</article>
<!-- END OF _includes/article.html -->

	
		
		<!-- START OF _includes/article.html -->
<article>
	<header>
		<h1><a href="/2018/03/21/tensorflow-for-tears-part-1/">TensorFlow For Tears: Part 1</a></h1>
		<time>21 March 2018</time>
	</header>
		<div class="content">
			<h3 id="an-introduction-to-every-parents-trial-and-travails">An introduction to every parent’s trial and travails</h3>

<p>When our son was born early last year, I admit I wasn’t ready for it. Fatherhood was not the kind of thing I was ready for (and who really can ever be ready for parenthood, anyways?).</p>

<p>It so turns out that the vast majority of the first year of parenting is simply enduring the gut-wrenching cries of your little one. And cry they do - crying when they are too tired, screaming when they are too energetic, crying when they are gassy, screaming when they are bored, and crying when they just pooped.</p>

<p>The trials that Annie and I went through with our little guy was particularly difficult on us (you can ask me in person if we ever get to chat). The little guy was a prolific screamer and absolutely. hated. sleep.</p>

<p>What’s a geeky dad to do? Quantify household suffering by leveraging machine learning, of course.</p>

<p>I set out to build a system that would in the end determine how well our little guy slept through (or didn’t sleep through) the night. I started by building a system that naively parsed audio samples from his nursery, and then trained a TensorFlow model to do more accurate detection of his cries. Here’s how it worked:</p>

<h3 id="act-1-the-misery-meter">Act 1: The Misery Meter</h3>

<p>In version 1 of the system, I bought <a href="https://www.amazon.com/Kinobo-Microphone-Desktop-Recognition-Software/dp/B00IR8R7WQ">a cheap USB microphone</a> and hooked it up to a Raspberry Pi 3.</p>

<p>In it, I loaded up a script to record a 30-second audio sample, using the <code>arecord</code> UNIX command line tool:</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line"><span class="c">#!/usr/bin/env bash</span>
</span><span class="line">
</span><span class="line"><span class="nb">set</span> -euxo pipefail
</span><span class="line">
</span><span class="line"><span class="nv">DIR</span><span class="o">=</span><span class="s2">&quot;$( cd &quot;</span><span class="k">$(</span> dirname <span class="s2">&quot;${BASH_SOURCE[0]}&quot;</span> <span class="k">)</span><span class="s2">&quot; &amp;&amp; pwd )&quot;</span>
</span><span class="line"><span class="nv">RECORDING_FILE</span><span class="o">=</span><span class="s2">&quot;${DIR}/recordings/sample.wav&quot;</span>
</span><span class="line"><span class="nv">DATE</span><span class="o">=</span><span class="k">$(</span>date <span class="s2">&quot;+%Y-%m-%d %H:%M:%S&quot;</span><span class="k">)</span>
</span><span class="line"><span class="nv">UPLOAD_RECORDING_FILENAME</span><span class="o">=</span><span class="k">$(</span><span class="nb">printf</span> %q <span class="s2">&quot;${DATE}.wav&quot;</span><span class="k">)</span>
</span><span class="line">arecord --device<span class="o">=</span>hw:1,0 --format S16_LE --rate <span class="m">48000</span> -c1 -d <span class="m">30</span> --quiet <span class="s2">&quot;${RECORDING_FILE}&quot;</span>
</span></code></pre></td></tr></table></div></figure>

<p>…then I ran the <code>sox</code> command-line tool to grab some simple loudness statistics out from it:</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">sox -V3 <span class="k">${</span><span class="nv">RECORDING_FILE</span><span class="k">}</span> -n stats 2&gt;<span class="p">&amp;</span><span class="m">1</span> <span class="p">|</span> grep dB
</span></code></pre></td></tr></table></div></figure>

<p>In case you’re curious, here’s the full output from <code>sox -V3 FILE -n stats</code>. Pretty nifty:</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">➜ sox -V3 sample.wav -n stats
</span><span class="line"><span class="c"># ... Truncated for brevity ...</span>
</span><span class="line">
</span><span class="line">sox INFO sox: effects chain: input        48000Hz  <span class="m">1</span> channels
</span><span class="line">sox INFO sox: effects chain: stats        48000Hz  <span class="m">1</span> channels
</span><span class="line">sox INFO sox: effects chain: output       48000Hz  <span class="m">1</span> channels
</span><span class="line">DC offset   0.000017
</span><span class="line">Min level  -0.141083
</span><span class="line">Max level   0.135651
</span><span class="line">Pk lev dB     -17.01
</span><span class="line">RMS lev dB    -29.32
</span><span class="line">RMS Pk dB     -27.37
</span><span class="line">RMS Tr dB     -30.83
</span><span class="line">Crest factor    4.12
</span><span class="line">Flat factor     0.00
</span><span class="line">Pk count           2
</span><span class="line">Bit-depth      14/16
</span><span class="line">Num samples     480k
</span><span class="line">Length s      10.000
</span><span class="line">Scale max   1.000000
</span><span class="line">Window s       0.050
</span></code></pre></td></tr></table></div></figure>

<p>Here, we’re really only interested in <code>RMS dB</code>, which is the relative loudness levels within this 30-second sample. I chose to push these three stats up to a web service which I use to aggregate and graph these metrics. <code>RMS lev</code> is the average, <code>RMS Pk</code> is the peak, and <code>RMS Tr</code> is the trough (the floor).</p>

<p>I’m not showing the entirety of the script, but the last thing it does is parse and push the results of the analysis of this audio sample to a Web-based metrics aggregation service! In case you were wondering, I have an API service sitting between the Raspberry Pi and a time-series API supplied by <a href="https://www.keen.io">Keen.io</a>. But the main point is that now I can load up a cute JS widget that graphs these data points!</p>

<p><img src="/images/tensorflow-for-tears/audio-graph.png" alt="Audio crying graph" /></p>

<p>Now, how does one read this graph?</p>

<ul>
  <li>We can follow the peaks of the audio signal and assume that any noise over a certain dB threshold is the little dude’s screaming.</li>
  <li>We can follow the troughs of the graph and assume that if the trough jumps, then man there is some serious crying going on, since the audio floor of the soundscape has been bumped up!</li>
  <li>Or we can follow the average RMS reading and assume some combination of the two?</li>
</ul>

<p>The truth of the matter, none of the readings from the Misery Meter (so I called it) were particularly reliable indicators of “the little buddy is crying his little head off”. Sometimes, his crying was at the same volume level as other ambient noises in the house (say, when he’s playing in the other room and someone shuts a door). So it turns out that using volume as a proxy for crying is insufficient to give us reliable results.</p>

<h3 id="act-2-enter-tensorflow-next">Act 2: Enter TensorFlow… next!</h3>

<p>In my next post, I’ll discuss how I modified this script to use TensorFlow to train a model that could then be used to enhance the accuracy of little dude’s crying. Stick around, it’ll be fun!</p>

<p><a href="/2018/08/27/tensorflow-for-tears-part-2/">Read on for the next post!</a></p>

			
			
		</div>
	<footer>
		


		



  
		
			Tagged under
		
    <a class='category' href='/category/audio-processing/'>audio processing</a>, <a class='category' href='/category/machine-learning/'>machine learning</a>, <a class='category' href='/category/parenting/'>parenting</a>, <a class='category' href='/category/tensorflow/'>tensorflow</a>
	


	
	
	</footer>
	
</article>
<!-- END OF _includes/article.html -->

	
		
		<!-- START OF _includes/article.html -->
<article>
	<header>
		<h1><a href="/2018/03/13/elixir-and-elm-things-ive-written-elsewhere/">Elixir and Elm things I've written about elsewhere</a></h1>
		<time>13 March 2018</time>
	</header>
		<div class="content">
			<p>While it’s been pretty quiet around these parts, I’ve kept the technical blogging up over on <a href="https://blog.carbonfive.com">my employer’s blog</a>. I’ve got quite a few posts around Elixir:</p>

<p><a href="https://blog.carbonfive.com/2018/03/19/lightweight-dependency-injection-in-elixir-without-the-tears/">Lightweight dependency injection in Elixir (without the tears)</a>: What are some ways we can use the language features of Elixir to apply small-scale dependency injection patterns, without a ton of ceremony?</p>

<p><a href="https://blog.carbonfive.com/2018/01/16/functional-mocks-with-mox-in-elixir/">Functional Mocks with Mox in Elixir</a>: This post discusses a mocking library in Elixir called <a href="https://github.com/plataformatec/mox">mox</a> that adheres to the Elixir Way(tm) of mocking (that is, the usage of fakes or doubles).</p>

<p><a href="https://blog.carbonfive.com/2018/01/30/comparing-dynamic-supervision-strategies-in-elixir-1-5-and-1-6/">Comparing Dynamic Supervision Strategies in Elixir 1.5 and 1.6</a>: In this blog post, I discuss the benefits of the new <code>DynamicSupervisor</code> module in Elixir 1.6 and how it makes the supervision of a dynamically-scaled supervision tree easier to set up.</p>

<p>Finally, I wrote <a href="https://blog.carbonfive.com/2017/10/25/taking-elm-for-a-test-drive/">“Taking Elm for a Test Drive”</a> about playing around with Elm, giving my take on the trajectory of the language.</p>

			
			
		</div>
	<footer>
		


		



  
		
			Tagged under
		
    <a class='category' href='/category/development/'>development</a>, <a class='category' href='/category/elixir/'>elixir</a>, <a class='category' href='/category/elm/'>elm</a>
	


	
	
	</footer>
	
</article>
<!-- END OF _includes/article.html -->

	
		
		<!-- START OF _includes/article.html -->
<article>
	<header>
		<h1><a href="/2017/07/19/pitfalls-to-avoid-when-moving-to-async-systems/">Pitfalls to avoid when moving to async systems</a></h1>
		<time>19 July 2017</time>
	</header>
		<div class="content">
			<p>I recently published a post on the <a href="http://blog.carbonfive.com">Carbon Five blog</a> titled <a href="http://blog.carbonfive.com/2017/07/18/evented-rails-decoupling-complex-domains-in-rails-with-domain-events/">“Evented Rails: Decoupling complex domains in Rails with Domain Events”</a> that takes some of my thoughts about moving a Rails app to use Domain Events - leveraging the power of Sidekiq (or your job runner of choice) to send async messages between different domains of your app.</p>

<p>This approach always seems nice from the outset, but can hide some painful complexities if you go too far down the rabbit hole. Here is a repost of the latter half of that article, which is worth repeating:</p>

<h2 id="big-wins-of-the-async-model-speed--scalability">Big win[s of the async model]: speed &amp; scalability</h2>

<p>By splitting out domain logic into cohesive units, we’ve just designed our systems to farm out their workloads to a greater scalable number of workers. Imagine if you had a web request thread that would take 500ms to return, but 150ms of that time was spent doing a round trip to a different service. By decoupling that work from the main request thread and moving it to a background job – we’ve just sped up the responsiveness of our system for our end user, and we know that studies have shown that page speed performance equals money.</p>

<p>Additionally, making our application calls asynchronous allows us to scale the number of processing power we allocate to our system. We now have the ability to horizontally scale workers according to the type of job, or the domain they are working from. This may result in cost and efficiency savings as we match processing power to the workload at hand.</p>

<h2 id="big-challenge-dealing-with-asynchronous-data-flows">Big challenge: dealing with asynchronous data flows</h2>

<p>Once things go async, we now have a fundamentally different data design. For example, say you implemented an HTTP API endpoint that performed some action in the system synchronously. However, now you’ve farmed out the effects of the action to background processes through domain events. While this is great for response times, you’ve now no longer got the guarantees to the caller that the desired side effect has been performed once the server responds back.</p>

<h3 id="asynchronous-polling">Asynchronous polling</h3>

<p>An option is to implement the Polling pattern. The API can return a request identifier back to the caller on first call, with which which the caller can now query the API for the result status. If the result is not ready, the API service will return with a Nack message, or negative Ack, implying that the result data has not arrived yet. As soon as the results in the HTTP API are ready, the API will correctly return the result.</p>

<h3 id="pubsub-all-the-way-down">Pub/Sub all the way down</h3>

<p>Another option is to embrace the asynchronous nature of the system wholly and transition the APIs to event-driven, message-based systems instead. In this paradigm, we would introduce an external message broker such as RabbitMQ to facilitate messages within our systems. Instead of providing an HTTP endpoint to perform an action, the API service could subscribe to a domain event from the calling system, perform its side effect, then fire off its own domain event, to which the calling system would subscribe to. The advantage of this approach is that this scheme makes more efficient use of the network (reducing chattiness), but we trade off the advantages of using HTTP (the ubiquity of the protocol, performance enhancements like layered caching).</p>

<p>Browser-based clients can also get in on the asynchronous fun with the use of WebSockets to subscribe to server events. Instead of having a browser call an HTTP API, the browser could simply fire a WebSocket event, to which the service would asynchronously process (potentially also proxying the message downstream to other APIs with messages) and then responding via a WebSocket message when the data is done processing.</p>

<h2 id="big-challenge-data-consistency">Big challenge: data consistency</h2>

<p>When we choose an asynchronous evented approach, we now have to consider how to model asynchronous transactions. Imagine that one domain process charges a user’s credit card with a third party payment processor and another domain process is responsible for updating it in your database. There are now two processes updating two data stores. A central tenet in distributed systems design is to anticipate and expect failure. Let’s imagine any of the following scenarios happens:</p>

<ol>
  <li>An Amazon AWS partial outage takes down one of your services but not the other.</li>
  <li>One of your services becomes backed up due to high traffic, and no longer can service new requests in a timely manner.</li>
  <li>A new deployment has introduced a data bug in a downstream API that your teams are rushing to fix, but will requiring manual reconciling with the data in the upstream system.</li>
</ol>

<p>How will you build your domain and data models to account for failures in each processing step? What would happen if you have one operation occur in one domain that depends on data that has not yet appeared in another part of the system? Can you design your data models (and database schema) to support independent updates without any dependencies? How will you handle the case when one domain action fails and the other completes?</p>

<h3 id="first-approach-avoid-it-by-choosing-noncritical-paths-to-decouple-first">First approach: avoid it by choosing noncritical paths to decouple, first</h3>

<p>If you are implementing an asynchronous, evented paradigm for the first time, I suggest you carefully begin decoupling boundaries with domain events only for events that lie outside the critical business domain path. Begin with some noncritical aspect of the system — for example, you may have a third party analytics tracking service that you must publish certain business events to. That would be a nice candidate to decouple from the main request process and move to an async path.</p>

<h3 id="second-approach-enforce-transactional-consistency-within-the-same-processdomain-boundary">Second approach: enforce transactional consistency within the same process/domain boundary</h3>

<p>Although we won’t discuss specifics in this article, if you must enforce transactional consistency in some part of your system (say, the charging of a credit card with the crediting of money to a user’s account) then I suggest that you perform those operations within the same bounded context and same process, leaning on transactional consistency guarantees provided by your database layer.</p>

<h3 id="third-approach-embrace-it-with-eventual-consistency">Third approach: embrace it with eventual consistency</h3>

<p>Alternatively, you may be able to lean on “eventual consistency” semantics with your data. Maybe it’s less important that your data squares away with itself immediately — maybe it’s more important that the data, at some guaranteed point in time — eventually lines up. It may be OK for some aspect of your data (e.g. notifications in a news feed) and may not be appropriate for other data (e.g. a bank account balance).</p>

<p>You may need to fortify your system to ensure that data eventually becomes consistent. This may involve building out the following pieces of infrastructure.</p>

<ol>
  <li>Messages need to be durable — make sure your job enqueuing system does not drop messages, or at least has a failure mode to re-process them when (not if!) your system fails.</li>
  <li>Your jobs should be designed to be idempotent, so they can be retried multiple times and result in the correct outcome.</li>
  <li>You should easily be able to recover from bad data scenarios. Should a service go down, it should be able to replay messages, logs, or the consumer should have a queue of retry-able messages it can send.</li>
  <li>Eventual consistency means that you may need an external process to verify consistency. You may be doing this sort of verification process in a data warehouse, or in a different software system that has a full view of all the data in your distributed system. Be sure that this sort of verification is able to reveal to you holes in the data, and provide actionable insights so you can fix them.</li>
  <li>You will need to add monitoring and logging to measure the failure modes of the system. When errors spike, or messages fail to send (events fail to fire), you need to be alerted. Once alerted, your logging must be good enough to be able to trace the source and the data that each request is firing.</li>
</ol>

<p>The scale of this subject is large and is under active research in the field of computer science. A good book to pick up that discusses this topic is <a href="https://www.amazon.com/Service-Oriented-Design-Rails-Addison-Wesley-Professional/dp/0321659368">Service-Oriented Design with Ruby on Rails</a>. The popular <a href="http://www.enterpriseintegrationpatterns.com/">Enterprise Integration Patterns</a> book also has a great topic on consistency (and is accompanied by a very helpful <a href="http://www.enterpriseintegrationpatterns.com/patterns/conversation/EnsuringConsistencyIntro.html">online guide</a> as well).</p>

			
			
		</div>
	<footer>
		


		



  
		
			Tagged under
		
    <a class='category' href='/category/ddd/'>ddd</a>, <a class='category' href='/category/microservices/'>microservices</a>, <a class='category' href='/category/rails/'>rails</a>
	


	
	
	</footer>
	
</article>
<!-- END OF _includes/article.html -->

	
		
		<!-- START OF _includes/article.html -->
<article>
	<header>
		<h1><a href="/2016/10/14/jhipster-and-spring-boot-for-rails-developers/">JHipster &amp; Spring Boot for Rails developers</a></h1>
		<time>14 October 2016</time>
	</header>
		<div class="content">
			<p>The first question you may be asking is - <em>why would I want to go from Rails to Java</em>?</p>

<p>Maybe you don’t have a choice. Maybe you started a new job. Maybe you heard Java was the new, old hotness. In any case, you’re a Ruby on Rails developer and you’re staring Java in the face.</p>

<h3 id="the-languages">The languages</h3>

<p>First off, Rails and Java share similar philosophies.</p>

<p>You may argue that Java is the One True Language for old-fashioned Object-Oriented Programming. The existence of strong types lead to powerful expressions around OOP concepts like inheritance, polymorphism, and the like.</p>

<p>Java and Ruby share similar philosophies - in that Everything Is An Object. In Ruby, the <code>nil</code> value is modeled as a <code>NilClass</code>. In Java, objects abound everywhere.</p>

<h3 id="the-frameworks">The frameworks</h3>

<p>I can’t speak from firsthand experience, but Java developers will tell you that developing Java apps in the early 2000s was like configuration soup. Everything was explicit and configured in XML.</p>

<p>Spring Boot arguable moves the state of the art in Java frameworks more towards’ Rails’ philosophies - that convention trumps configuration. It accomplishes this through <em>annotations</em> - more on this later.</p>

<p>Rails was precisely so groundbreaking and exciting in 2005 because it was everything Java was not - terse, expressive, and unapologetically convention-driven. Where in Java, everything was explicitly traceable through system calls, Rails used magic methods of dynamically-defined methods, monkey-patching and big ol’ global God Objects to accomplish its magic.</p>

<h3 id="liquibase-vs-active-record">Liquibase vs Active Record</h3>

<p>In Active Record, database changes are called <em>migrations</em>.</p>

<p>Migrations are only run from migration files, and may optionally be generated from the CLI.</p>

<p>In Liquibase, these are called <em>changesets</em>, and the files are called <em>changelogs</em>.</p>

<p>These migrations are either generated from the Liquibase CLI, or there is a nifty tool that reads Hibernate persistence entities and generates a “diff” against a known database, writing the migration to a file.</p>

<p>Rails checks in a <code>schema.rb</code> file, encompassing the canonical definition of the DB schema. There is no such equivalent in Liquibase (* I may be wrong).</p>

<p><strong>To be continued…</strong></p>

			
			
		</div>
	<footer>
		


		



  
		
			Tagged under
		
    <a class='category' href='/category/java/'>java</a>, <a class='category' href='/category/liquibase/'>liquibase</a>, <a class='category' href='/category/ruby-on-rails/'>ruby on rails</a>, <a class='category' href='/category/spring-boot/'>spring boot</a>
	


	
	
	</footer>
	
</article>
<!-- END OF _includes/article.html -->

	
	
<div class="pagination">
	
		<a class="older" href="/posts/2">Older</a>
	
	
	<span class="total">1 of 21</span>
	
	
</div>
<!-- END OF index.html -->


			<footer>
				Copyright &copy; 2018

	Andrew Hao


				

<script type="text/javascript">
      var disqus_shortname = 'g9labs';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





			</footer>

		</div> <!-- // .wrap -->
  </body>

	<script>
		$(document).ready(function() {
			// Make images center
			$('p:has(img)').css('text-align', 'center');

			// Add the image's title attribute as a caption
			$('p:has(img)').append(function () {
				return '<div class="caption">' + ($('img', this).attr('title') || "") + '</div>';
			});

			// Prettify code
			$('code').addClass('prettyprint');
			$('pre code').addClass('linenums');

			// Copy to clipboard with button
			$('pre:has(code)').prepend(function(){
				return '<div class="clip-btn">copy to clipboard</div>';
      });

			$('.clip-btn').zclip({
				path:'/js/ZeroClipboard.swf',
				copy: $(this).next('code').text(),
				afterCopy: function(){
					$(this).replaceWith('<div class="clip-btn">copied!');
					}
			});
		});
	</script>
</html>
<!-- END OF _layouts/default.html -->
