<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: rails | The Sweet Spot]]></title>
  <link href="http://www.g9labs.com/category/rails/atom.xml" rel="self"/>
  <link href="http://www.g9labs.com/"/>
  <updated>2016-06-23T17:31:40-07:00</updated>
  <id>http://www.g9labs.com/</id>
  <author>
    <name><![CDATA[Andrew Hao]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Notes on performance tuning a Puma server]]></title>
    <link href="http://www.g9labs.com/2015/06/29/notes-on-performance-tuning-a-puma-server/"/>
    <updated>2015-06-29T11:47:00-07:00</updated>
    <id>http://www.g9labs.com/2015/06/29/notes-on-performance-tuning-a-puma-server</id>
    <content type="html"><![CDATA[<p>A couple of months ago, I was tuning a Rails app for one of our clients.
This client wanted to know how performant their app would be under load.</p>

<p>To do that, you can do several different things:</p>

<ol>
<li>Tune the thread/process balance within the VM</li>
<li>Horizontally scale with your cloud platform.</li>
</ol>


<p>This is a discussion of the former (#1):</p>

<h2>1) Set up the test</h2>

<h3>Drive with a synthetic script</h3>

<p>Our application had a synthetic load driver that would run Selenium to
execute various app tasks. This synthetic driver could be parallelized
across many notes via Rainforest QA, Sauce Labs or Browserify.</p>

<p>In our case, I only needed to run our synthetic load script on a single
node in multiple processes, which simulated enough load to anticipate
another order of magnitude of traffic.</p>

<h3>Know how to inspect the server under load.</h3>

<p>Commands you will want to know:</p>

<pre><code>$ free -m # Find the total amount of free memory on your machine
$ ps uH p &lt;pid&gt; # List out process threads
$ kill -TTIN &lt;puma_master_pid&gt; # Add a puma worker
$ kill -TTOU &lt;puma_master_pid&gt; # Remove a puma worker
$ kill -USR2 &lt;puma_master_pid&gt; # Kill the puma master &amp; workers
</code></pre>

<h2>Generating more load: use external load testing services, or plain tools.</h2>

<p>Try using <a href="http://www.flood.io">Flood.io</a> or JMeter for performance load.</p>

<p>I tried looking into the <a href="https://github.com/schneems/puma_auto_tune">puma_auto_tune</a> gem, but it required a higher level of production instrumentation than I was ready to give it.</p>

<h2>Analysis: New Relic scalability analysis</h2>

<p>New Relic gave us a scalability analysis scatter plot, plotting
throughput against average application response time. In essence, it
allows you to see spikes in response times as correlated to throughput.</p>

<h2>Process:</h2>

<p>My approach was to use the synthetic script to generate productionlike
node and ramp up the # of load actors in 5m increments. Each run would
test the following Puma process/thread balance:</p>

<p>Run #1: Single-process, multi threads.
Run #2: Multiple processes, single threaded.
Run #3: Multiple processes, multiple threads.</p>

<blockquote><h3>Aside: <em>how many</em> of these threads/processes should I be using?</h3>

<p>Note that your numbers will be different on the execution
characteristics of your app and your server environment. Tweak it for
yourself. You&rsquo;re designing an experiment.</p>

<p>If you&rsquo;re curious, our Rails app started out with 4 threads on 2
workers. We made the # of Puma workers (both min and max) environment
variables so we could tweak the variables easily without deploying.</p></blockquote>

<p>The strategy was then to look at the perf characteristics of each run in
the scatter plot. If there were any spikes in the graph with the
increase of load, then that would be noted. Even minor features like an
increase in slope would be noted &ndash; at that point, the incremental cost
of each request increases with overall system load.</p>

<h2>Results</h2>

<p>I don&rsquo;t have the New Relic data on hand to show, now, but in our case we
discovered two things:</p>

<ol>
<li>The server easily scaled from ~10 &ndash;> ~500 rpm with a virtually flat
line for all runs.</li>
<li>The app exhibited no noticeable performance differences when flipped
between uniprocess-multithreaded, multiprocess-unithreaded, and
multiprocess-multithreaded modes. Any performance gains were under a
tolerable threshold.</li>
</ol>


<p>How do we parse these results?</p>

<ul>
<li>We note that we didn&rsquo;t really push the performance threshold on this
app (it&rsquo;s not meant to be a public web site and 95% of it is behind a
login wall to a specialized group of users). Thus, if we pushed the
concurrent connections even more, we may have seen more of a pronounced
difference.</li>
<li>The <em>absence</em> of any major red flags was itself a validation. The
question we wanted answered coming into this experiment was &ldquo;how close
are we to maxing out our single-node EC2 configuration such that we will
have to begin configuring horizontal scaling?&rdquo;? The answer was: we can
safely scale further out in the near-term future, and cross the bridge
of horizontal scaling/bursting when we get there.</li>
<li>We did not have enough statistically significant differences in
performance for #threads/#processes in Puma. However, if we wanted to
truly find the optimal performance in our app, we would have turned to
tools like <a href="https://github.com/schneems/puma_auto_tune">puma_auto_tune</a> to answer those questions.</li>
</ul>


<p>Let me know in the comments if you have any questions!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ember Data, Rails, CORS, and you!]]></title>
    <link href="http://www.g9labs.com/2013/12/27/ember-data-rails-cors-and-you/"/>
    <updated>2013-12-27T16:22:00-08:00</updated>
    <id>http://www.g9labs.com/2013/12/27/ember-data-rails-cors-and-you</id>
    <content type="html"><![CDATA[<p>I&rsquo;m starting up a new personal project involving Ember-Data and Rails
(more to come). The gist of it is that it&rsquo;s a pure frontend app engine
built in Yeoman and Grunt, and designed to talk to a remote API service
built on Rails.</p>

<p>So since it&rsquo;s a remote API, I&rsquo;ve got to enable CORS, right?</p>

<h2>Install CORS via rack-cors</h2>

<p><code>ruby Gemfile.rb
gem "rack-cors", :require =&gt; "rack/cors"
</code></p>

<p>```ruby config/application.rb
config.middleware.use Rack::Cors do
  allow do</p>

<pre><code>origins "*"

resource "*",
  :headers =&gt; :any,
  :methods =&gt; [:get, :post, :put, :delete, :options, :patch]
end
</code></pre>

<p>  allow do</p>

<pre><code>origins "*"
resource "/public/*",
  :headers =&gt; :any,
  :methods =&gt; :get
</code></pre>

<p>  end
end
```</p>

<p>A very naive implementation with zero security whatsoever. Anyways.
Onward!</p>

<h2>Get Ember-Data DS.RESTAdapter talkin' CORS</h2>

<p>I saw conflicting documentation on Ember-Data and CORS &mdash; it seemed like
it should support CORS out of the box. Apparently this is not so.</p>

<p>In my ember app&rsquo;s <code>store.js</code> (or anywhere your app loads before the
application adapter is defined, do this:</p>

<p>```javascript store.js
$.ajaxSetup({
  crossDomain: true,
  xhrFields: {</p>

<pre><code>withCredentials: true
</code></pre>

<p>  }
});</p>

<p>Hendrix.Store = DS.Store.extend();
Hendrix.ApplicationAdapter = DS.RESTAdapter.extend({
  host: &ldquo;<a href="http://localhost:3000">http://localhost:3000</a>&rdquo;,
})
```</p>

<p><a href="http://api.jquery.com/jQuery.ajaxSetup/"><code>$.ajaxSetup</code></a>, though its
usage is not recommended, is designed to set global options on the
jQuery <code>ajax</code> object. It provides some information on the options you can modify.</p>

<p>Why doesn&rsquo;t Ember support this out of the box? I think it&rsquo;s because they
cannot support IE, where one must use an XDR object to support CORS.</p>

<p>I&rsquo;ve posted an <a href="http://discuss.emberjs.com/t/ember-data-and-cors/3690">Ember follow-up question in the
forums</a> for discussion.</p>

<h2>Get Rails talking JSON out of its mimetype confusion.</h2>

<p>Did you know that if you rely on the <code>Accepts:</code> header in HTTP that
Rails does not obey its ordering<code>*</code>? I was trying to figure out why my
Rails controllers were trying to render HTML instead of JSON when the
headers were:</p>

<p><code>'Accept: application/json, text/javascript, */*; q=0.01'</code></p>

<p>A <a href="https://github.com/rails/rails/issues/9940">very long winded
discussion</a> on the Rails
project reveals that, well, nobody has it figured out yet. Most modern
browsers do obey <code>Accepts:</code> specificity, but for the sake of older
browser compatibility, the best practice for browsers is still to return
HTML when <code>*/*</code> is specified.</p>

<p>What does this mean for Rails developers who want to use <code>Accepts:</code>
mimetype lists? Well, we either wait for the Rails projects to support
mimetype specificity (and for older browsers to die out), or we are
encouraged to include the format explicitly in the URI.</p>

<p>I chose to have Ember append the <code>.json</code> suffix to the URL, thanks to
this <a href="http://stackoverflow.com/questions/13648807/ds-model-url-not-working-in-ember-js">SO
post</a></p>

<p><code>``javascript store.js
Hendrix.ApplicationAdapter = DS.RESTAdapter.extend({
  host: "http://localhost:3000",
  // Force ember-data to append the</code>json` suffix
  buildURL: function(record, suffix) {</p>

<pre><code>return this._super(record, suffix) + ".json";
</code></pre>

<p>  }
})
```</p>

<p>More to come how how this app works.</p>
]]></content>
  </entry>
  
</feed>
